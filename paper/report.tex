\documentclass[11pt,a4paper]{article}
\usepackage[utf8x]{inputenc}
\usepackage{ucs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fancyvrb}
\usepackage{url}
\usepackage{graphicx}

\author{HU, Pili and LI, Yichao}
\title{Community Detection on 2-Hop Topology}

\newcommand{\question}{\textbf{---NEED-REVIEW-HERE---}}

\begin{document}

\maketitle

\begin{abstract}
	Community detection in social networks has drawn great interest
	in recent decades, especially inspired by the emerge of wide 
	online social network sites. In this paper, we study the two-hop 
	topology from several observers in the "renren.com" network. 
	Some important statistics are calculated first, along with 
	some visualization. Then we compute mining(results)
	 \question

	This document serves as the report of CSCI5180 course 
	project\cite{csci5180_lecture,csci5180_tutorial}. 
	For more information, codes, and data, please refer to 
	our open source repository\cite{hu2011-cd2hop}. 
	
\end{abstract}

\pagebreak
\tableofcontents
\pagebreak

\section{Introduction}

Community detection has drawn great interest in the near decade. 
The explosion of online Social Network Sites make the study more 
practical but challenging. Traditional methodology aims at 
optimizing certain global metric. The resultant graph partition 
can thus be regarded as communities. The motivation of detecting 
communities on 2-hop topology comes in two folds: 
\begin{enumerate}
	\item \textbf{Data Availability}. The global topology of 
	those SNS is kept confidential. However, users are allowed 
	to view their buddies profile by default. This gives us the 
	chance to construct a 2-hop subgraph from the whole graph. 
	Take a step further, outsiders can always observe some forwarding 
	sequences of tweet/status. This helps to reconstruct a 
	subgraph more than 2-hop from the observer. The methodology 
	developed here can thus be extended to such situations. 
	\item \textbf{Computation Availability}. Although those SNS 
	providers have access to the global topology, it is computationally
	intractable to perform some well-developed global optimization. 
	Local heuristics are widely used in many applications. 
\end{enumerate}

The potential application may be:
\begin{enumerate}
	\item \textbf{Friend recommendation}. 
	\item \textbf{Targeted advertisement}. 
\end{enumerate}

\section{Data Preparation and Preprocessing}

\subsection{Data Source}

The data source used in this paper is "renren.com", which is the 
most popular SNS in mainland China. At the mean while, "renren.com" 
has the following good characteristics:
\begin{itemize}
	\item Web UI of "3g.renren.com" is very clean. Cawling through
	this interface doesn't require any API calls.  
	\item Renren is a real name dense graph connected mainly through 
	real communities. Intuitively speaking, 
	people who are close to the observer should be observed within 
	this 2-hop topology with high probability. 
	\item Institution label for all nodes in the observed subgraph
	is available on "renren.com". It makes model validation easier
	and automated. If the model is proven effective, we can adapt 
	it to other SNS's, where institution label may be not available.  
\end{itemize}

\subsection{Crawler}

The crawler is developed using the following components:
\begin{itemize}
	\item \begin{verbatim}
		perl
	\end{verbatim}
	\item \begin{verbatim}
		wget
	\end{verbatim}
	\item \begin{verbatim}
		bash
	\end{verbatim}
\end{itemize}

We fake HTTP login and page requests like an ordinary user. 
Random delay between two adjacent requests is introduced 
to mitigate the automation pattern. 

For more details, interested readers are recommended to 
our project \cite{hu2011-cd2hop}. The crawler is under 
directory 'crawler', together with detailed instructions 
on how to deploy one yourself. 

\subsection{Preprocessing}

The preprocessing steps is as follows:
\begin{enumerate}
	\item Parse crawled web pages to get user ID and institution name. 
	\item Create one vertex for each person crawled. 
	\item Create one edge for each friend relationship. 
	\item Do isomorphic tranformation on the original graph
	\cite{wiki_iso_graph}. Vertex number and insitution label 
	is shuffled according to a psudorandom sequence. 
	This is to protect privacy of our volunteer observers. 
\end{enumerate}

After preprocessing, we get the raw data, which is in the
following format:
\begin{Verbatim}
---'link'---
NodeID1 '\t' NodeID2
NodeID1 '\t' NodeID3
...

---'node'---
NodeID1 '\t' Label1
NodeID2 '\t' Label2
...
\end{Verbatim}

To facilitate our model training and evaluation, 
we divide our data into two parts randomly and equally: 
one training set and one testing set. 

\section{Notations}

The notations used throughout this paper are gathered 
in table(\ref{tbl:notation}). 

\begin{table}[htb]
	\centering
	\caption{Notations}
	\label{tbl:notation}
	\begin{tabular}{c|c}
	\hline
	Symbol & Explanation \\
	\hline
	$n$ & node number \\
	$m$ & edge number \\
	$A$ & adjacency matrix \\
	$A_{ij}$ & 1 if node $i$ and node $j$ are directly \\
	& connected, 0 otherwise\\
	$N(i)$ & neighbourhood of $i$, namely \\
	& $N(i) = \{j | A_{ij} = 1 \}$ \\
	$d(i)$ & degree of node $i$ \\
	& $d(i) = \sum_{j}{A_{ij}}$ \\
	$o$ & observer \\
	$\overrightarrow{v}$ & ranking vector of nodes \\
	\hline
	\end{tabular}
\end{table}


\section{Statistics}

\section{Feature Selection}

\subsection{Random Walk Based Techniques}

The rationale behind random walk is: the nearer a node
is to observer, it can be reached with a higher probability by a random 
	walker starting from observer. PageRank is one of the most classical work 
	in this field.

We shortlist three variations\cite{aggarwal2011social} of the original 
PageRank, together with the explanation of using them in our context:
		\begin{itemize}
			\item PageRank with escape probability:
			\begin{equation}
				\overrightarrow{v} = \alpha A^{\rm T}\overrightarrow{v}
				+ (1-\alpha)\frac{\overrightarrow{1}}{n}
				\label{eq:pr}
			\end{equation}
			where $A$ is the transition matrix, typically the adjacency matrix,
			and $\alpha$ is the transfer ratio. $ \overrightarrow{v} $ is 
			the resultant PageRank value. 
			
			In our graph, the link is very sparse for edge node, which 
			represents the friend of observer's friend. They provide 
			many dangling links, which influences the outcome of 
			original PageRank algorithm. These nodes may appear as probability
			sinks. By introducing in escape probability, such sinks can be eliminated. 
			
			Our graph is rooted and thus biased at the observer. So the
			rank output by this algorithm can be a proximity 
			measure between other nodes and observer. 
			
			\item Personalized PageRank:
			\begin{equation}
				\overrightarrow{v} = \alpha A^{\rm T}\overrightarrow{v}
				+ (1-\alpha)\frac{\overrightarrow{b}}{||\overrightarrow{b}||_1}
				\label{eq:ppr}
			\end{equation}
			The difference from eqn(\ref{eq:pr}) is that, the all one's column vector 
			$\overrightarrow{1}$ is 
			substituted by a more general weight vector, also called the 
			escape vector. This vector describes where and by what probability 
			the random walker will escape to. 		
			
			With different escape vector $\overrightarrow{b}$, we can reach 
			different goal:
				\begin{itemize}
					\item Unsupervised learning. Denote the subscript of observer
					as $o$, then:
					\begin{equation}
						b_i=\lbrace{\begin{tabular}{cc}
							1 & $i=o$ \\
							0 & $i\neq o$ 
						\end{tabular}}
					\end{equation}
					That means the random walker will always escape to the root node. 
					Then the output can measure the proximity between root and other nodes. 
					\item Semi-supervised learning. Denote the labeled set as 
					$L$ and their label as $l_i$. The personalized escape 
					vector can be:
					\begin{equation}
						b_i=\lbrace{\begin{tabular}{cc}
							$l_i$ & $i \in L $ \\
							$0$ & $i\notin L $ 
						\end{tabular}}
					\end{equation}
					Typically, the miner can choose a subset of the nodes, which are
					already known to share the same label with the observer. We assign
					those nodes in set $L$ some positive weights. Then the random walker 
					can escape to this set with different probability. The rantionale is, 
					if one node is in our community, it is probable to be reached 
					from some already known members.  
				\end{itemize}

		\end{itemize}  
		
\subsection{Simple Proximity Measures}

Yet random walk based techniques are good proximity measures in many context, 
there exists some simple but effective measures. We shortlist the following 
metrics, together with explanation of using them in our context:
	\begin{itemize}
		\item Common Neighbours:
		\begin{equation}
			Common(i,j) = | N(i) \cap N(j) |
			\label{eq:common}
		\end{equation}
		It is straight forward that if node $i$ shares more common neighbours 
		with observer $o$, it is closer to observer. 
		
		\item Adamic/Adar score:
%change the equation back
%no need to do "1" opeartion, since this divide-by-zero flaw results
%from the situation where i==j, and one of the neighbours only have 
%degree 1. 
%=== 
%		\begin{equation}
%			Adamic/Adar(i,j) = \sum_{k \in N(i) \cap N(j) }{\frac{1}{\log{|N(k)|+1}}}
%			\label{eq:adar}
%		\end{equation}
%===
		\begin{equation}
			Adamic/Adar(i,j) = \sum_{k \in N(i) \cap N(j) }{\frac{1}{\log{|N(k)|}}}
			\label{eq:adar}
		\end{equation}
		Adamic/Adar can be seen as an extension of simple common neighbours, which 
		take the neighbour's degree into consideration. The contribution from 
%		each common neighbour $k$ is weighted as $\frac{1}{\log{|N(k)|+1}}$. 
		each common neighbour $k$ is weighted as $\frac{1}{\log{|N(k)|}}$. 
		Higher degree nodes may stant for public pages, or very well-known people 
		in the network. Sharing such kind of common neighbour is a much weaker 
		evidence that two people are in the same community. Thus the it is given 
		a lower weight. The use of log scale stems from previous statistical studies, 
		that node ranking tends to be Zipf distributed. \cite{breslau1999web-zipf}
		
		\item Jaccard's coefficient:
		\begin{equation}
			J(i,j)=\frac{|N(i) \cap N(j) |}{|N(i) \cup N(j) |}
			\label{jaccards}
		\end{equation}
		This coefficient can be seen as another extension of common neighbours. 
		It is effective when the graph is sparse. The numerator calculates the 
		common neighbours of two nodes. The denominator takes their own size
		into consideration. It's reasonable that two nodes with higher degree 
		will have more common neighbours. The denominator act as a normalizer, 
		which makes the output of different node pairs relative comparable. 
	\end{itemize}

\section{Single Feature Evalutaion}

\subsection{Quality Functions}

After we label the community of all nodes, we can calculate some quality 
functions to indicate how well the community is detected. Different researchers
have different preference of quality functions, to name a few:
	\begin{itemize}
		\item Normalized cut:
		\begin{equation}
			Ncut(S)=\frac{\sum_{i \in S, j \in \overline{S}}{A(i,j)}}
			{\sum_{i \in S}{d(i)}}
			+ \frac{\sum_{i \in S, j \in \overline{S}}{A(i,j)}}
			{\sum_{j \in \overline{S}}{d(j)}}					
			\label{eq:ncut}	
		\end{equation}
		where $S$ is the set of one community, 
		and $d(.)$ is the digree of a node, which is defined as 
		$d(i)=\sum_j{A_{ij}}$.
		
		The smaller the value, the better community detection quality. 
		Two numerators measure the number of inter community links, 
		and the value is normalized by the number of intra community links. 
		\item Conductance:
		\begin{equation}
			Conductance(S)=\frac{\sum_{i \in S, j \in \overline{S}}{A(i,j)}}
			{\min \{ \sum_{i \in S}{d(i)}, \sum_{j \in \overline{S}}{d(j)}\}}	
			\label{eq:conductance}			
		\end{equation}
		This quality function is positive related with eqn(\ref{eq:ncut}). 
		When community sizes are highly skewed, conductance may provide 
		better evaluation. 
		\item Modularity:
		\begin{equation}
			Q=\sum_{i=1}^{K}{\left[ 
			\frac{A(V_i,V_i)}{m} 
			-\left( \frac{d(V_i)}{2m}\right)^2
			\right]}
			\label{eq:modularity}
		\end{equation}
		where 
		\begin{equation}
			A(V_i,V_j)= \sum_{u \in V_i,v \in V_j}{A_{uv}}
		\end{equation}
		and 
		\begin{equation}
			d(V_i)=\sum_{u \in V_i}{d_u}
		\end{equation}
		$K$ is the number of communities, which is 2 in our case
		(we only distinguish whether the node is in the same community
		with observer or not).
		
		The rationale behind modularity is the comparison with a random 
		graph. The father resultant community is from a random graph, 
		the better the quality. "One of the advantages of modularity is 
		that it is independent of the number of communities the graph is
		divided into"\cite{aggarwal2011social}. 
	\end{itemize}
	
	Those quality functions are popular among different research 
	groups. They also capture different characteristics of graphs. 
	In this project, we'll check if these global metric can 
	be used to evaluate our extreme local case. 


\subsection{Receiver Operating Characteristics}

Draw the Receiver Operating Curve by varying proximity threshold. 
			(Different TP and FP rate)




\question


\section{Model Training and Testing}

%=====================data=====

%observer 2, full nodes. MLP, default. 
%=== Detailed Accuracy By Class ===
%
%               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
%                 0.383     0.031      0.599     0.383     0.468      0.746    1
%                 0.969     0.617      0.929     0.969     0.949      0.746    0
%Weighted Avg.    0.906     0.554      0.894     0.906     0.897      0.746
%
%=== Confusion Matrix ===
%
%     a     b   <-- classified as
%  1812  2915 |     a = 1
%  1212 38180 |     b = 0

%observer 2, cut degree leq1. MLP, default. 
%
%=== Detailed Accuracy By Class ===
%
%               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
%                 0.534     0.202      0.699     0.534     0.606      0.72     1
%                 0.798     0.466      0.661     0.798     0.723      0.72     0
%Weighted Avg.    0.675     0.342      0.679     0.675     0.668      0.72 
%
%=== Confusion Matrix ===
%
%    a    b   <-- classified as
% 1382 1205 |    a = 1
%  595 2348 |    b = 0
%  %

%observer 2, cut degree leq2. MLP, default.
%=== Detailed Accuracy By Class ===
%
%               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
%                 0.987     0.916      0.639     0.987     0.776      0.69     1
%                 0.084     0.013      0.793     0.084     0.151      0.69     0
%Weighted Avg.    0.645     0.575      0.697     0.645     0.54       0.69 
%
%=== Confusion Matrix ===
%
%    a    b   <-- classified as
% 1706   23 |    a = 1
%  963   88 |    b = 0


%observer 2, only level 1. MLP, default
%=== Detailed Accuracy By Class ===
%
%               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
%                 0.685     0.173      0.813     0.685     0.744      0.801    1
%                 0.827     0.315      0.704     0.827     0.761      0.801    0
%Weighted Avg.    0.752     0.241      0.761     0.752     0.752      0.801
%
%=== Confusion Matrix ===
%
%  a  b   <-- classified as
% 74 34 |  a = 1
% 17 81 |  b = 0

%==============================data end ================

\subsection{MLP with Full Topology}

Our first experiment is to train an MLP with full topology. 
Let observer 2 be an example, the confusion matrix on testing 
data is shown in table(\ref{tbl:cm_o2_full}). 

\begin{table}[htb]
	\centering
	\caption{Confusion Matrix: MLP, Observer2, Full Topology}
	\label{tbl:cm_o2_full}
	\begin{tabular}{cc|c}
	\hline
a & b & \textbf{$\leftarrow$ classified as}\\	
	\hline
1812 &  2915 &    $a = 1$ \\
1212 & 38180 &    $b = 0$\\
	\hline
	\end{tabular}
\end{table}

The precision is 0.599 and recall is 0.383. The overall accuracy 
is 0.906. Although the overall accuracy is very high, we can 
conclude that this classification is of low application value. 
The True Negative population is very large, which means our trained 
MLP tends to classify nodes into class 0. 

One of our targeted use of the predicted labels can be friend 
recommendation. It's nonsense to recommend a lot of friends 
at a time. So our application is not sensitive to recall rate. 
Thus we work towards higher precision in the successive experiments, 
and at the same time avoid making the predication trivial. 

\subsection{MLP without Degree 1 Nodes}

\question

According to our previous statistics, large amount of nodes
are of degree one. This doesn't necessarily mean the real 
degree, but means the degree within our observation. 
The 2-hop topology is so limited that we see many leaf nodes. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.5\linewidth]{fig/illustration_2hop_level2.jpg}
	\caption{Illustration of Two Levels}
	\label{fig:illu_l2}
\end{figure}

Fig(\ref{fig:illu_l2}) illustrates one part of the 2-hop 
topology. The real friend relationship is bidirectional. 
The arrows in the illustration just represents the direction 
of our crawler. Nodes n5 and n6 stand for two typical situations 
of those level 2 nodes. It's very probable that n6 with single link 
from a level 1 node is not in the observer's community. 
On the contrary, n5, which many level 1 nodes point to is very likely
to be in the same community as observer. 

Nodes like n6 will bring up priori probability of class 0 as shown in 
the previous experiment. We eliminate all nodes with degree 1, and 
run the training stage again. Result confusion matrix is shown 
in table(\ref{tbl:cm_o2_d1}). 

\begin{table}[htb]
	\centering
	\caption{Confusion Matrix: MLP, Observer2, without Degree 1}
	\label{tbl:cm_o2_d1}
	\begin{tabular}{cc|c}
	\hline
a & b & \textbf{$\leftarrow$ classified as}\\	
	\hline
 1382 &1205 &    $a = 1$\\
  595 &2348 &    $b = 0$\\
	\hline
	\end{tabular}
\end{table}

The precision is now 0.699 and recall is  0.534. 
Both metrics are improved, while the overall accuracy 
degrades to 0.675. Bearing our potential application in mind, 
this result is better, which means if we target 10 nodes 
from the predicted positive class, we may have a chance
of 70\% to hit observer's community. 

\subsection{MLP without Degree 2}

Take the last experiment further, it's rational to 
think whether eliminating more nodes can help. 

We train the same model after eliminating degree 2 nodes. 
The result confusion matrix is shown in table().

\begin{table}[htb]
	\centering
	\caption{Confusion Matrix: MLP, Observer2, without Degree 2}
	\label{tbl:cm_o2_d2}
	\begin{tabular}{cc|c}
	\hline
a & b & \textbf{$\leftarrow$ classified as}\\	
	\hline
 1706 &23 &    $a = 1$\\
  963 &88 &    $b = 0$\\
	\hline
	\end{tabular}
\end{table}

The precision is now 0.639 and recall is 0.987. 
Though we get a very good recall rate, the precision 
is lowered. Looking into the confusion matrix, 
we find this classification becomes \textbf{trivial}. 
It tends to predict class 1. Even a naive guess 
that classifies all nodes into class 1 can reach 
the precision of 0.62194 and recall 1.0. 

With more lower degree nodes being eliminated, 
we may think this problem becomes more significant. 
Since the priori probability of class 1 increases, 
the naive guess precision can increase by keeping 
a 100\% recall. 

\subsection{MLP with Only Level 1 Nodes}

If we limit our mining scope to only level 1 nodes, 
the application can not be friend recommendation, since 
they have alreay established one link with observer. 
Nevertheless, taking a look at the result is of great interest. 
table(\ref{tbl:cm_o2_l1}) shows the result confusion matrix. 

\begin{table}[htb]
	\centering
	\caption{Confusion Matrix: MLP, Observer2, Only Level 1 Nodes}
	\label{tbl:cm_o2_l1}
	\begin{tabular}{cc|c}
	\hline
a & b & \textbf{$\leftarrow$ classified as}\\	
	\hline
 74 &34 &    $a = 1$\\
  17 &81 &    $b = 0$\\
	\hline
	\end{tabular}
\end{table}

The precision is 0.813, and the recall is 0.685. 
If we aim at the "community" defined as a group 
of people sharing the same interest, rather than 
those institutions, this result suggests targeted 
advertisement can be performed effeciently. 


\section{Conclusion}


\section{Future Work}

As is in our proposal \cite{hu2011-cd2hop}, there are other metrics
we want to try. Due to project time and report space, we only name 
a few in this section:
\begin{itemize}
	\item Katz Score:
			\begin{equation}
				Katz(i,j)=\sum_{l=1}^{\infty}{\beta^lA^l(i,j)}
			\end{equation}
			The matrix series results in 
			\begin{equation}
				Katz = (I-\beta A)^{-1} - I
			\end{equation}
			By tuning $\beta$, this score can measure number of paths between 
			two nodes, with preferred length range. 
	\item Simrank:
			\begin{equation}
				s(a,b)=\frac{\gamma}{|I(a)||I(b)|}
				\sum_{i \in I(a), j \in I(b)} s(i,j)
			\end{equation}
			This metric measures the expectation of $\gamma^l$, where $l$ equals 
			the time when two random walkers start from $a$ and $b$ meet. By proper 
			matrix construction, Simrank can be solved as an eigenvalue problem. 
\end{itemize}
For more information, interested readers are recommended to \cite{aggarwal2011social}. 

The links used in this study is mere static topology. 
With 2-hop limit, the information can be extracted 
is not abundant. In terms of data source, we propose 
two promising directions:
\begin{enumerate}
	\item Dynamic link information. "renren.com" is a very 
	dense graph. However, many edges are not effetive in the sense
	that little information flow through them. We can 
	build links using user interactions like status forwarding, 
	blog reply, etc. 
	\item Content based prediction. Note that links can only capture
	static topology or dynamic interaction, but not content. Nodes in a 
	well-defined community share similar content with very high 
	probability. Combining topology and content may be a promising 
	direction. 
\end{enumerate}

Similar thoughts have been proposed in many related literature, to name 
a few\cite{boccaletti2006complex,aggarwal2011social}. 

\section*{Acknowledgements}

The authors would like to thank the following people for their contribution of 
personal data: 
Bo WANG, Hongshu LIAO, Huan CHEN, Shouxi LUO, 
Xiaoyu LUO, Xinyue ZHENG, Yan WANG, Yi LUO. 
The authors appreciate brain storm discussion with professor Wing Lau 
and Deyi Sun. 
The authors would like to thank course instructors and TAs of CSCI5180. 


%begin================bibliography======================

%this seems impossible to merge automatically
%at the same time, I don't want to construct .bib databse
%for all these temporary entries. 
%
%my solution is to extract bibentries and paste them in 
%the following {thebibliography} section
%
%
%%\bibliographystyle{plain}
%%\bibliography{mylibrary}

%%%%GENBIBSTRING%%%%

\begin{thebibliography}{99}

%cut and paste section 
%generate by 'make bib' from my bib database
\bibitem{hu2011-cd2hop}
Hu~Pili and Li~Yichao.
\newblock Community detection on 2-hop topology.
\newblock GitHub, https://github.com/Czlyc/2C-Web-Research, 12 2011.
\newblock course project of CUHK/CSCI5180.

\bibitem{aggarwal2011social}
C.C. Aggarwal.
\newblock {\em Social network data analytics}.
\newblock Springer-Verlag New York Inc, 2011.

\bibitem{breslau1999web-zipf}
L.~Breslau, P.~Cao, L.~Fan, G.~Phillips, and S.~Shenker.
\newblock Web caching and zipf-like distributions: Evidence and implications.
\newblock In {\em INFOCOM'99. Eighteenth Annual Joint Conference of the IEEE
  Computer and Communications Societies. Proceedings. IEEE}, volume~1, pages
  126--134. IEEE, 1999.

\bibitem{boccaletti2006complex}
S.~Boccaletti, V.~Latora, Y.~Moreno, M.~Chavez, and D.U. Hwang.
\newblock Complex networks: Structure and dynamics.
\newblock {\em Physics reports}, 424(4-5):175--308, 2006.


%temporary section
	\bibitem{csci5180_lecture} CSCI5180, Data Mining Lecture Notes, 
		\url{http://www.cse.cuhk.edu.hk/~lwchan/teaching/csc5180.html}
	\bibitem{csci5180_tutorial} CSCI5180, Data Mining Tutorial Notes. 
		(available in Moodle. Please contact the teacher/TA if you have 
		interest)
	\bibitem{wiki_roc} Receiver Operating Characteristics, 
		\url{http://en.wikipedia.org/wiki/Receiver_operating_characteristic}
	\bibitem{weka} Weka, 
		\url{http://www.cs.waikato.ac.nz/ml/weka/index.html}
	\bibitem{wiki_iso_graph} Wikipedia, Isomorphic Graph, 
		\url{http://en.wikipedia.org/wiki/Graph_isomorphism}
\end{thebibliography}

%end================bibliography======================


\section*{Appendix}


\section*{Declaration}

Meta tools like PageRank, Adamic/Adar, etc, are learned from the book
\textit{Social Network Data Analytics}. For original sources of these 
algorithms, please refer to the bibliography pages of that book.

The analysis/adaptation/extension/application of these tools and 
the use in community detection on 2-hop topology mentioned 
in this document is originality. Please refer to our open source 
repository for citation issues.

The data in our project repository is limited to research use only. 
The authors of this paper own the transformed version. You are welcome
to use them in your study but remember to make a notification to the 
authors first. Our contact information is given in the project 'README' file. 

\end{document}
